import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class PSNR(nn.Module):
    """
    Implementation of PSNR Calculation for image tensors.
    """
    def __init__(self, maximum_pixel_value=1.0): 
        """
        Initializes the PSNR Calculation module. 

        Args: 
            maximum_pixel_value (float): maximum pixel value of the input tensors for PSNR calculation.
        """
        super().__init__()
        self.me_loss = nn.MSELoss()
        self.max_value = maximum_pixel_value

    def forward(self, clean_img, sr_img): 
        """
        Computes the Peak Signal-to-Noise Ratio (PSNR) between the clean (reference) image and the 
        super-resolved (SR) image. 

        Args:
            clean_img (torch.Tensor): The ground truth image.
            sr_img (torch.Tensor): The super-resolved image generated by the model.

        Returns:
            torch.Tensor: The PSNR value.
        """
        mse_loss = self.me_loss(clean_img, sr_img) 
        psnr_value = 10 * torch.log10(self.max_value ** 2 / mse_loss)
        return psnr_value

class SSIM(nn.Module):
    def __init__(self, constants=(0.0001, 0.0001)):
        """
        Initializes the SSIM calculation Module. 

        Args: 
            constants (Tuple[Int, Int]): Constants (C1, C2) for stablizing SSIM calculation.
        """
        super().__init__()
        self.C1 = constants[0]
        self.C2 = constants[1]

    def channel_calculation(self, x, y):
        """
        Computes SSIM for a single channel of the images.

        Args:
            x (torch.Tensor): The first image channel.
            y (torch.Tensor): The second image channel.

        Returns:
            torch.Tensor: The SSIM value for the given channel.
        """
        mean_x = torch.mean(x)
        mean_y = torch.mean(y)
        
        variance_x = torch.var(x, unbiased=False)
        variance_y = torch.var(y, unbiased=False)
        
        covariance_xy = torch.mean((x - mean_x) * (y - mean_y))
        
        SSIM = ((2 * mean_x * mean_y + self.C1) * (2 * covariance_xy + self.C2)) / ((mean_x ** 2 + mean_y ** 2 + self.C1) * (variance_x + variance_y + self.C2))
        
        return SSIM

    def channel_splits(self, x, y):
        """
        Splits the images into individual channels and returns them as separate tensors.

        Args:
            x (torch.Tensor): The first image tensor with shape (N, C, H, W).
            y (torch.Tensor): The second image tensor with shape (N, C, H, W).

        Returns:
            (List[torch.Tensor], List[torch.Tensor]): Two lists containing the channels of x and y, respectively.
        """
        assert x.shape[1] == y.shape[1], "[ERROR] X and Y have different number of channels."

        x_tensors = []
        y_tensors = []

        for i in range(x.shape[1]):
            x_tensors.append(x[:, i, :, :])
            y_tensors.append(y[:, i, :, :])
        
        return x_tensors, y_tensors
    
    def forward(self, clean_img, sr_img):
        """
        Computes Structural Similarity Index (SSIM) between the clean (reference) image and the 
        super-resolved (SR) image.

        Args: 
            clean_img (torch.Tensor): The ground truth image.
            sr_img (torch.Tensor): The super-resolved image generated by the model.
        
        Returns: 
            torch.Tensor: The SSIM value

        """
        x_tensors, y_tensors = self.channel_splits(clean_img, sr_img)

        SSIM_values = 0.0
        for x, y in zip(x_tensors, y_tensors):
            SSIM_values += self.channel_calculation(x, y)
        
        return 1 - SSIM_values / len(x_tensors) 
    
class GradientPriorLoss(nn.Module):
    def __init__(self):
        """
        Initializes Gradient Prior Loss Calculation module.
        """
        super(GradientPriorLoss, self).__init__()
        self.func = nn.L1Loss()

    def forward(self, clean_img, sr_img):
        """
        Computes the gradient prior loss between the clean (reference) image and the 
        super-resolved (SR) image. This loss is based on the difference in gradient maps of the images.

        Args:
            clean_img (torch.Tensor): The ground truth image.
            sr_img (torch.Tensor): The super-resolved image generated by the model.

        Returns:
            torch.Tensor: The gradient prior loss value.
        """
        map_clean = self.gradient_map(clean_img)
        map_sr = self.gradient_map(sr_img)
        
        return self.func(map_clean, map_sr)

    def gradient_map(self, x):
        """
        Computes the gradient map of an image. The gradient map is calculated using the differences
        between adjacent pixels in horizontal and vertical directions.

        Args:
            x (torch.Tensor): The input image tensor with shape (N, C, H, W).

        Returns:
            torch.Tensor: The gradient map of the input image.
        """
        _, _, h_x, w_x = x.size()

        r = F.pad(x, (0, 1, 0, 0))[:, :, :, 1:]
        l = F.pad(x, (1, 0, 0, 0))[:, :, :, :w_x]
        t = F.pad(x, (0, 0, 1, 0))[:, :, :h_x, :]
        b = F.pad(x, (0, 0, 0, 1))[:, :, 1:, :]  

        xgrad = torch.pow(torch.pow((r - l) * 0.5, 2) + torch.pow((t - b) * 0.5, 2) + 1e-6, 0.5)
        
        return xgrad
    
class MSE_Loss(nn.Module):
    def __init__(self): 
        """
        Initializes MSE Calculation module.
        """
        super.__init__()

    def forward(self, x, y): 
        """
        Compute the Mean Square Error (MSE) Loss between x and y 
        
        Args: 
            x (torch.Tensor): The x tensor containing the same dimensions as y 
            y (torch.Tensor): The y tensor containing the same dimensions as x
        
        Returns:
            torch.Tensor: The Mean Squared Error Loss between x and y.

        """
        difference = x - y 
        return torch.mean(difference ** 2)

def get_optimizer(model, optimizer_config):
    """
    Returns the optimizer specified in the optimizer_config dictionary for the given model.

    Parameters:
    - model: The neural network model whose parameters will be optimized.
    - optimizer_config: Dictionary containing the configuration for the optimizer.
      Expected keys:
      - 'name': The name of the optimizer (e.g., 'Adam', 'SGD').
      - 'lr': (Optional) Learning rate for the optimizer. Default is 1e-3.
      - 'momentum': (Optional, for 'SGD' only) Momentum factor. Default is 0.9.

    Returns:
    - An instance of the specified optimizer.

    Raises:
    - ValueError: If an unknown optimizer name is provided.
    """
    optimizer_name = optimizer_config['name']
    learning_rate = optimizer_config.get('lr', 1e-3)

    if optimizer_name == 'Adam':
        return optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        return optim.SGD(model.parameters(), lr=learning_rate, momentum=optimizer_config.get('momentum', 0.9))
    else:
        raise ValueError(f"[ERROR] Unknown optimizer name: {optimizer_name}")

def get_scheduler(optimizer, scheduler_config):
    """
    Returns the learning rate scheduler specified in the scheduler_config dictionary for the given optimizer.

    Parameters:
    - optimizer: The optimizer whose learning rate will be scheduled.
    - scheduler_config: Dictionary containing the configuration for the scheduler.
      Expected keys:
      - 'name': The name of the scheduler (e.g., 'StepLR', 'CosineAnnealingLR').
      - 'step_size': (Optional, for 'StepLR' only) Period of learning rate decay. Default is 30.
      - 'eta_min' : (Optional, for 'CosineAnnealingLR' only) Minimum learning rate to decay to. 
      - 'gamma': (Optional, for 'StepLR' only) Multiplicative factor of learning rate decay. Default is 0.1.
      - 'T_max': (Optional, for 'CosineAnnealingLR' only) Maximum number of iterations. Default is 50.

    Returns:
    - An instance of the specified scheduler.

    Raises:
    - ValueError: If an unknown scheduler name is provided.
    """
    scheduler_name = scheduler_config['name']

    if scheduler_name == 'StepLR':
        return optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_config.get('step_size', 30), gamma=scheduler_config.get('gamma', 0.1))
    elif scheduler_name == 'CosineAnnealingLR':
        return optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=scheduler_config.get('eta_min', 1e-6), T_max=scheduler_config.get('T_max', 50))
    else:
        raise ValueError(f"[ERROR] Unknown scheduler name: {scheduler_name}")